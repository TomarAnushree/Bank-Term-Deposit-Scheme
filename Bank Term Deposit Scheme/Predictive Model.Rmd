---
title: "Bank Term Deposit Scheme Predictive Model"
author: "Anushree Tomar"
date: "7-10-2021"
output:
  pdf_document: default
  html_notebook: default
---



# Import Required Libraries
```{r include=FALSE}
library('data.table')
library('devtools')
library('remotes')
#remotes::install_github("dongyuanwu/RSBID")
library(RSBID)
#devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
library('catboost')
library('CatEncoders')
library('ggplot2')
library('lattice')
library('caret')
library('rpart')
library('rattle')
library('rpart.plot')
library('xgboost')
library(ROCR)
```

# Import Files 

First read train and test data sets:-
```{r}
train<-fread("G:\\Anushree G4\\hachthon\\Bank Term Deposit\\Complete-Data-Set\\Training_Dataset_Time_Deposit - Sheet1.csv",stringsAsFactors = T,na.strings = "")
test<-fread("G:\\Anushree G4\\hachthon\\Bank Term Deposit\\Complete-Data-Set\\Testing_Dataset_Time_Deposit - Sheet1 (1).csv",stringsAsFactors = T,na.strings = "")

Sample_Submission<-fread("G:\\Anushree G4\\hachthon\\Bank Term Deposit\\Complete-Data-Set\\Sample Submission Updated - Sheet1.csv")
```

# Binning of Age 

In this step we will create groups of the age attribute and label them as 0-4, 5-9, 10-14 and so on.


```{r}
#Binning of Age 
label <- c(paste(seq(0, 95, by = 5), seq(0 + 5 - 1, 100 - 1, by = 5),
                sep = "-"), paste(100, "+", sep = ""))
train$age <- cut(train$age, breaks = c(seq(0, 100, by = 5), Inf), labels = label, right = FALSE)

test$age <- cut(test$age, breaks = c(seq(0, 100, by = 5), Inf), labels = label, right = FALSE)

# Drop duration

#train<-train[,-'duration']
#test<-test[,-'duration']
str(test)


```

# Creating balanced data

```{r}
load("G:/Anushree G4/hachthon/Bank Term Deposit/Bank Term Deposit Scheme/bal_100.RData")
#balanced_data <-  SMOTE_NC(train[,-'key'], 'y', perc_maj=100) 
```

```{r}
str(balanced_data)
```
# Re-Leveling of Attributes

We found new levels in the test data so that to solve our problem we releveled the train data as the test dataset.  
```{r}
levels(balanced_data$education)<-levels(test$education)
levels(balanced_data$default)<-levels(test$default)
levels(balanced_data$month)<-levels(test$month)
```

# Data Pre-processing for Catboost model

# Encoding categorical variables
```{r}
cat_data<-balanced_data[, lapply(balanced_data, class) == 'factor', with = FALSE]
cont_data<-balanced_data[,lapply(balanced_data, class) != 'factor', with = FALSE]
cat_col <- colnames(cat_data)


encode <- sapply(cat_data, function(x) LabelEncoder.fit(x))
for (i in cat_col){
    cat_data[[i]] <- transform(encode[[i]], balanced_data[[i]])
}

cat_data <- cbind(cat_data, cont_data)
```


```{r}
str(cat_data)
```

# Data Partition

Let's partitioned the 70% of data into traindata and rest into the testdata. 

```{r}
#cat_data <- cbind(cat_data, target)
trainindex<-createDataPartition(cat_data$y,p=0.7,list=F)
traindata<-cat_data[trainindex,]
testdata<-cat_data[-trainindex,]
```


# Create train/test pools from train(balanced data) data
```{r}
y_train <- traindata[,"y"]
X_train <- traindata[,-'y'] 

y_test <- testdata[,"y"]
X_test <- testdata[,-"y"]

train_pool <- catboost.load_pool(data = X_train, label = y_train)
test_pool <- catboost.load_pool(data = X_test, label = y_test)
```

# Build  Catboost Model
# Used overfitting detector for more faster training
```{r}

params_simple <- list(iterations = 500,
                      learning_rate=0.001,
                      depth=4,
                      loss_function = 'Logloss',
                      eval_metric='Logloss',
                      random_seed = 55,
                      od_type='Iter',
                      metric_period = 50,
                      od_wait=30,
                      use_best_model=TRUE,
                      logging_level = 'Silent')


model_simple <- catboost.train(train_pool, test_pool, params_simple)
# Load Saveed model
#model_simple <- catboost::catboost.load_model("model")
cat('Model with Tuned Parameter tree count: ', model_simple$tree_count, '\n')
```

# Visualize important features
```{r}
feat_imp<-catboost.get_feature_importance(model_simple)
feat_imp<-data.frame('Feature' = rownames(feat_imp), 'Importance' =feat_imp[,1])
feat_imp<-feat_imp[order(feat_imp$Importance,decreasing = T),]
ggplot(feat_imp, aes(x = Feature, y = Importance,fill=Feature)) +geom_bar(stat='identity') +
theme(axis.text.x= element_text(angle = 45)) +scale_x_discrete(limits = feat_imp$Feature)+theme(legend.position = "none")
```


# Prediction on testdata
### Confusion Matrix
```{r}

preds <- catboost.predict(model_simple, test_pool,prediction_type = 'Class')
# In train and test data one hot code as 1,2
y_test<-ifelse(y_test==1,"0","1")
#table(preds, testdata[,y])
confusionMatrix(factor(y_test),factor(preds),mode="everything",positive = "1")
```

# Encoding categorical variables of final test data
```{r}
cat_data<-test[, lapply(test, class) == 'factor', with = FALSE]
cont_data<-test[,lapply(test, class) != 'factor', with = FALSE]
cont_data<-cont_data[,-'key']
cat_col <- colnames(cat_data)


encode <- sapply(cat_data, function(x) LabelEncoder.fit(x))
for (i in cat_col){
    cat_data[[i]] <- transform(encode[[i]], test[[i]])
}

cat_data <- cbind(cat_data, cont_data)

final_test<-catboost.load_pool(cat_data)

```


# Prediction on final test
```{r}
preds <- catboost.predict(model_simple, final_test,prediction_type = 'Class')
#preds<-ifelse(preds==0,"no","yes")
```

```{r}
submission1<-data.frame("key"=Sample_Submission$key,"y"=preds)
#write.csv(submission,"catboost4.9lrleveltuned_binage.csv",row.names = F,quote = F)

#output<-data.frame("Modelname"=c("catboost1","catboost2_no_smote","catboost3_binage","catboost4tuned_binage","catboost4.1tuned_binage","catboost4.2tuned_binage","catboost4.3tuned_binage","catboost4.4tuned_binage","catboost4.54tuned_binage","catboost4.6tuned_binage","catboost4.7tuned_binage","catboost4.9lrleveltuned_binage","relevel_4","XGboost_new1","XGboost_new2"),"Score"=c(63.9395,53.84,64.0991,63.264,63.6113,65.2701,65.3935,65.0935,61.7049,67.5373,67.1413,67.6076,67.7178,61.433,64.6261))

#write.csv(output,"models_score.csv",row.names = F,quote = F)
```

# save model
```{r}

#catboost.save_model(model_simple, "model")


#model_simple <- catboost::catboost.load_model("model")


```

# Test of loaded model
```{r}

#preds <- catboost.predict(model2, test_pool,prediction_type = 'Class')
#preds<-ifelse(preds==0,"1","2")
#confusionMatrix(factor(testdata[,y]),factor(preds))
```

# Decision Tree Model

# Data partitioning
```{r}
trainindex<-createDataPartition(balanced_data$y,p=0.7,list=F)
traindata1<-balanced_data[trainindex,]
testdata1<-balanced_data[-trainindex,]
```

```{r}
data_tree <- readRDS("./data_tree_relevel4.rds")
#data_tree <- rpart(y~., method = "class", data = traindata1)
summary(data_tree)
# Find cp value of min xerror
#data_tree <- rpart(y~., method = "class", data = traindata1,control = rpart.control(minsplit=3,cp=.01))# min xerror

summary(data_tree)
```

# Decision Tree
```{r,echo=FALSE}

fancyRpartPlot(data_tree)
```


Cp results
```{r,echo=FALSE}
printcp(data_tree)
```

# Making Prediction on test data
```{r,echo=FALSE}
prob_final <- predict(data_tree,testdata1[,-"y"])
class_final <- data.frame("y"=predict(data_tree,testdata1[,-"y"],type="class"))
table(class_final)
```

# Confusion Matrix
```{r,echo=FALSE}

confusionMatrix(table(class_final$y,testdata1$y),positive = "yes",mode="everything")

```

# Making Prediction on final test data
```{r,echo=FALSE}
prob_final <- predict(data_tree,test[,-"key"])
class_final <- data.frame("y"=predict(data_tree,test[,-"key"],type="class"))
class_final$y<-ifelse(class_final$y=="no",'0','1')
submission2<-data.frame("key"=Sample_Submission$key,"y"=class_final$y)

#write.csv(submission,"relevel_4.csv",row.names = F,quote = F)


```


# Save and Load model
```{r}
# save the model to disk
#saveRDS(data_tree, "./data_tree_relevel4.rds")
 

 
# load the model
#super_model <- readRDS("./data_tree_relevel4.rds")
#prob_final <- predict(super_model,test[,-"key"])
#class_final <- data.frame("y"=predict(super_model,test[,-"key"],type="class"))
#table(class_final)

```




# XgBoost Model
```{r,echo=FALSE}
#Prepare Matrix
traindata[,"y"]<-ifelse(traindata[,"y"]==1,"0","1")
dtrain <- xgb.DMatrix(as.matrix(traindata[,-"y"]), label = as.matrix(traindata[,'y']))
testdata[,"y"]<-ifelse(testdata[,"y"]==1,"0","1")
dtest<-xgb.DMatrix(as.matrix(testdata[,-"y"]), label = as.matrix(testdata[,'y']))


dtestfinal<-xgb.DMatrix(as.matrix(cat_data))



#using grid search par
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.4, max_depth=15, min_child_weight=0.5, subsample=1, colsample_bytree=0.5)



#find best nround

cv<-xgb.cv( params = params, data = dtrain, nrounds = 75, nfold = 5,gamma=4, showsd = T, stratified = T, print_every_n = 10, early_stopping_rounds = 20, maximize = F)

cv$best_iteration
print(cv)
print(cv, verbose=TRUE)

#saveRDS(xgb1, "XGboost_new2_mod.rds")
xgb1<-readRDS("./XGboost_new2_mod.rds")

# uncomment if you want to re run the model

#xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100, watchlist = list(val=dtest,train=dtrain), print_every_n = 10, early_stopping_rounds = 20,gamma=4, maximize = F , eval_metric = "error")


```

# Prediction on test data
```{r}
#model prediction
 xgbpred <- predict (xgb1,dtest)
 xgbpred <- ifelse (xgbpred > 0.5,"1","0")
xgbpred<-data.frame("y"= xgbpred)
 
  xgbpredfinal <- predict(xgb1,dtestfinal)
  xgbpredfinal <- ifelse(xgbpredfinal > 0.5,"1","0")
  xgbpredfinal<-data.frame("y"= xgbpredfinal)
  Xgboostpred<-cbind(test[,"key"], xgbpredfinal)
  submission3<-Xgboostpred
# write.csv(Xgboostpred,"XGboost_new2.csv",row.names = FALSE,quote = F)
 
 #saveRDS(xgb1, "XGboost_new2_mod.rds")
```

# Confusion Matrix
```{r}
confusionMatrix(factor(xgbpred$y),factor(testdata$y),positive = "1",mode="everything")

```



```{r}
importance <- xgb.importance(feature_names = colnames(dtrain), model = xgb1)
head(importance)
xgb.plot.importance(importance_matrix = importance)
```


# Final Model 

We selected Decision Tree Model as our Final model based on the leaderboard score. 

